#!/usr/bin/env python
"""
Execution script for evaluating a model with two-simulator planning on ManiSkill2.
"""

import os
import argparse
import numpy as np
from collections import defaultdict

# Import your existing model classes
from your_model_path import YourDynamicsModel, OpenVLAModel

# Import the planning algorithm
from planning_algorithm import TwoSimulatorPlanner, PlanningModel

# Import your environment utilities
from simpler_env.utils.env.env_builder import build_maniskill2_env, get_robot_control_mode
from simpler_env.utils.env.observation_utils import get_image_from_maniskill2_obs_dict
from simpler_env.utils.visualization import write_video


def parse_args():
    parser = argparse.ArgumentParser(description="Run evaluation with two-simulator planning")
    
    # Environment settings
    parser.add_argument("--env_name", type=str, required=True, help="Environment name")
    parser.add_argument("--scene_name", type=str, required=True, help="Scene name")
    parser.add_argument("--robot", type=str, required=True, help="Robot name")
    
    # Model settings
    parser.add_argument("--model_path", type=str, required=True, help="Path to dynamics model")
    parser.add_argument("--openvla_path", type=str, required=True, help="Path to OpenVLA model")
    parser.add_argument("--ckpt_path", type=str, default="./checkpoints", help="Checkpoint path")
    
    # Planning algorithm hyperparameters
    parser.add_argument("--num_initial_actions", type=int, default=10, help="Number of initial actions (A)")
    parser.add_argument("--horizon", type=int, default=5, help="Horizon per action")
    parser.add_argument("--steps_ahead", type=int, default=3, help="Number of steps to look ahead (h)")
    parser.add_argument("--num_candidates", type=int, default=5, help="Number of candidate actions")
    parser.add_argument("--num_best_actions", type=int, default=3, help="Number of best actions")
    parser.add_argument("--temperature", type=float, default=1.0, help="Temperature for sampling")
    parser.add_argument("--render_tree", action="store_true", help="Whether to render the tree")
    
    # Robot and object initialization
    parser.add_argument("--robot_init_xs", nargs="+", type=float, default=[0.0], help="Robot initial x positions")
    parser.add_argument("--robot_init_ys", nargs="+", type=float, default=[0.0], help="Robot initial y positions")
    parser.add_argument("--robot_init_quats", nargs="+", type=float, default=[1, 0, 0, 0], help="Robot initial quaternions")
    parser.add_argument("--obj_variation_mode", type=str, default="episode", choices=["xy", "episode"], help="Object variation mode")
    parser.add_argument("--obj_init_xs", nargs="+", type=float, default=[], help="Object initial x positions")
    parser.add_argument("--obj_init_ys", nargs="+", type=float, default=[], help="Object initial y positions")
    parser.add_argument("--obj_episode_range", nargs=2, type=int, default=[0, 1], help="Object episode range")
    
    # Environment settings
    parser.add_argument("--control_freq", type=int, default=3, help="Control frequency")
    parser.add_argument("--sim_freq", type=int, default=513, help="Simulation frequency")
    parser.add_argument("--max_episode_steps", type=int, default=80, help="Maximum episode steps")
    parser.add_argument("--enable_raytracing", action="store_true", help="Enable raytracing")
    parser.add_argument("--additional_env_build_kwargs", type=str, default="{}", help="Additional environment build arguments")
    parser.add_argument("--additional_env_save_tags", type=str, default=None, help="Additional environment save tags")
    parser.add_argument("--obs_camera_name", type=str, default=None, help="Observation camera name")
    parser.add_argument("--rgb_overlay_path", type=str, default=None, help="RGB overlay path")
    parser.add_argument("--logging_dir", type=str, default="./results", help="Logging directory")
    
    args = parser.parse_args()
    
    # Parse lists and dicts from string arguments
    args.additional_env_build_kwargs = eval(args.additional_env_build_kwargs)
    
    # Parse quaternions
    if len(args.robot_init_quats) == 4:
        args.robot_init_quats = [args.robot_init_quats]
    else:
        args.robot_init_quats = [args.robot_init_quats[i:i+4] for i in range(0, len(args.robot_init_quats), 4)]
    
    return args


def create_reward_function():
    """
    Create a reward function for the planning algorithm.
    
    Returns:
        function: The reward function (state, action) -> reward
    """
    def reward_function(state, action=None):
        # Extract state information
        if "obs" in state:
            obs = state["obs"]
            
            # Extract gripper position
            if "agent" in obs and "proprioceptive" in obs["agent"]:
                gripper_pos = obs["agent"]["proprioceptive"].get("tcp_position", None)
            else:
                gripper_pos = None
            
            # Extract object position
            if "object" in obs and "position" in obs["object"]:
                object_pos = obs["object"]["position"]
            else:
                object_pos = None
            
            # Extract plate/target position
            if "goal" in obs and "position" in obs["goal"]:
                target_pos = obs["goal"]["position"]
            else:
                target_pos = None
            
            # Check if object is grabbed
            # This depends on your environment's state representation
            is_grabbed = False
            if "extra" in obs and "is_object_grasped" in obs["extra"]:
                is_grabbed = obs["extra"]["is_object_grasped"]
            
            # Calculate reward
            if gripper_pos is not None and object_pos is not None:
                # Calculate distance between gripper and object
                distance = np.linalg.norm(np.array(gripper_pos) - np.array(object_pos))
                
                # If object is grabbed, measure distance to target
                if is_grabbed and target_pos is not None:
                    distance = np.linalg.norm(np.array(object_pos) - np.array(target_pos))
                
                # Convert distance to reward (closer is better)
                return -distance
        
        # Default reward if we can't extract necessary information
        return 0.0
    
    return reward_function


def run_maniskill2_eval_with_planning(
    model_class,
    openvla_model_class,
    args
):
    """
    Run ManiSkill2 evaluation with the two-simulator planning algorithm.
    
    Args:
        model_class: Class for the dynamics model (second simulator)
        openvla_model_class: Class for the OpenVLA model
        args: Command-line arguments
    
    Returns:
        list: Success results for all episodes
    """
    control_mode = get_robot_control_mode(args.robot, "policy")
    success_arr = []
    
    # Initialize models
    dynamics_model = model_class(args.model_path)
    openvla_model = openvla_model_class(args.openvla_path)
    
    # Create reward function
    reward_function = create_reward_function()
    
    # Run evaluation for each robot initial position
    for robot_init_x in args.robot_init_xs:
        for robot_init_y in args.robot_init_ys:
            for robot_init_quat in args.robot_init_quats:
                # Common arguments for evaluation
                common_kwargs = dict(
                    ckpt_path=args.ckpt_path,
                    robot_name=args.robot,
                    env_name=args.env_name,
                    scene_name=args.scene_name,
                    robot_init_x=robot_init_x,
                    robot_init_y=robot_init_y,
                    robot_init_quat=robot_init_quat,
                    control_mode=control_mode,
                    additional_env_build_kwargs=args.additional_env_build_kwargs,
                    rgb_overlay_path=args.rgb_overlay_path,
                    control_freq=args.control_freq,
                    sim_freq=args.sim_freq,
                    max_episode_steps=args.max_episode_steps,
                    enable_raytracing=args.enable_raytracing,
                    additional_env_save_tags=args.additional_env_save_tags,
                    obs_camera_name=args.obs_camera_name,
                    logging_dir=args.logging_dir,
                )
                
                # Run for each object variation
                if args.obj_variation_mode == "xy":
                    for obj_init_x in args.obj_init_xs:
                        for obj_init_y in args.obj_init_ys:
                            # Create environment
                            env_kwargs = dict(
                                obs_mode="rgbd",
                                robot=args.robot,
                                sim_freq=args.sim_freq,
                                control_mode=control_mode,
                                control_freq=args.control_freq,
                                max_episode_steps=args.max_episode_steps,
                                scene_name=args.scene_name,
                                camera_cfgs={"add_segmentation": True},
                                rgb_overlay_path=args.rgb_overlay_path,
                            )
                            env = build_maniskill2_env(args.env_name, **env_kwargs, **args.additional_env_build_kwargs)
                            
                            # Create planning model
                            planning_model = PlanningModel(
                                env=env,
                                model=dynamics_model,
                                openvla_model=openvla_model,
                                reward_function=reward_function,
                                num_initial_actions=args.num_initial_actions,
                                horizon_per_action=args.horizon,
                                num_steps_ahead=args.steps_ahead,
                                num_candidates=args.num_candidates,
                                num_best_actions=args.num_best_actions,
                                temperature=args.temperature,
                                render_tree=args.render_tree,
                                logging_dir=os.path.join(args.logging_dir, "planning"),
                            )
                            
                            success = run_maniskill2_eval_single_episode(
                                model=planning_model,
                                obj_init_x=obj_init_x,
                                obj_init_y=obj_init_y,
                                **common_kwargs,
                            )
                            success_arr.append(success)
                
                elif args.obj_variation_mode == "episode":
                    for obj_episode_id in range(args.obj_episode_range[0], args.obj_episode_range[1]):
                        # Create environment
                        env_kwargs = dict(
                            obs_mode="rgbd",
                            robot=args.robot,
                            sim_freq=args.sim_freq,
                            control_mode=control_mode,
                            control_freq=args.control_freq,
                            max_episode_steps=args.max_episode_steps,
                            scene_name=args.scene_name,
                            camera_cfgs={"add_segmentation": True},
                            rgb_overlay_path=args.rgb_overlay_path,
                        )
                        env = build_maniskill2_env(args.env_name, **env_kwargs, **args.additional_env_build_kwargs)
                        
                        # Create planning model
                        planning_model = PlanningModel(
                            env=env,
                            model=dynamics_model,
                            openvla_model=openvla_model,
                            reward_function=reward_function,
                            num_initial_actions=args.num_initial_actions,
                            horizon_per_action=args.horizon,
                            num_steps_ahead=args.steps_ahead,
                            num_candidates=args.num_candidates,
                            num_best_actions=args.num_best_actions,
                            temperature=args.temperature,
                            render_tree=args.render_tree,
                            logging_dir=os.path.join(args.logging_dir, "planning"),
                        )
                        
                        success = run_maniskill2_eval_single_episode(
                            model=planning_model,
                            obj_episode_id=obj_episode_id,
                            **common_kwargs,
                        )
                        success_arr.append(success)
    
    return success_arr


def run_maniskill2_eval_single_episode(
    model,
    ckpt_path,
    robot_name,
    env_name,
    scene_name,
    robot_init_x,
    robot_init_y,
    robot_init_quat,
    control_mode,
    obj_init_x=None,
    obj_init_y=None,
    obj_episode_id=None,
    additional_env_build_kwargs=None,
    rgb_overlay_path=None,
    obs_camera_name=None,
    control_freq=3,
    sim_freq=513,
    max_episode_steps=80,
    instruction=None,
    enable_raytracing=False,
    additional_env_save_tags=None,
    logging_dir="./results",
):
    """
    Run a single episode of ManiSkill2 evaluation.
    
    This is the same function from your existing code, copied here for convenience.
    """
    # This function should be the exact same as the one from your paste.txt
    # I'm not including the full implementation here to avoid redundancy
    # You should either copy the function here or import it from your existing code
    
    # For brevity, I'll just include a placeholder:
    print("Running evaluation for a single episode with planning...")
    # Replace with actual implementation
    
    # Return a dummy success flag
    return True


def main():
    """
    Main function to run evaluation with planning.
    """
    args = parse_args()
    
    # Replace these with your actual model classes
    # These are placeholders
    class YourDynamicsModel:
        def __init__(self, model_path):
            self.model_path = model_path
        
        def reset(self, task_description):
            pass
        
        def step(self, image, task_description, override_action=None):
            # Return dummy actions
            raw_action = {"action": "raw"}
            processed_action = {
                "world_vector": np.zeros(3),
                "rot_axangle": np.zeros(4),
                "gripper": np.zeros(1),
                "terminate_episode": np.zeros(1),
            }
            return raw_action, processed_action
        
        def simulate(self, state, action):
            # Dummy simulation
            next_state = state
            reward = 0.0
            done = False
            truncated = False
            info = {}
            return next_state, reward, done, truncated, info
        
        def visualize_epoch(self, actions, images, save_path=None):
            pass
    
    class YourOpenVLAModel:
        def __init__(self, model_path):
            self.model_path = model_path
        
        def sample_actions(self, image, task_description, num_samples=1, temperature=1.0):
            # Return dummy actions
            return [{"action": f"action_{i}"} for i in range(num_samples)]
    
    # Run evaluation
    success_arr = run_maniskill2_eval_with_planning(
        model_class=YourDynamicsModel,
        openvla_model_class=YourOpenVLAModel,
        args=args,
    )
    
    # Print results
    num_success = sum(success_arr)
    num_total = len(success_arr)
    success_rate = num_success / num_total if num_total > 0 else 0
    
    print(f"Success rate: {success_rate:.2f} ({num_success}/{num_total})")


if __name__ == "__main__":
    main()